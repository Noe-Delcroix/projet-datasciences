{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRRQ8RLAQRiS",
        "outputId": "0671ee6b-2d56-4945-e4ad-265a231e75bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Epoch [1/20], Loss: 2.1700\n",
            "Epoch [2/20], Loss: 1.8041\n",
            "Epoch [3/20], Loss: 0.8446\n",
            "Epoch [4/20], Loss: 1.3096\n",
            "Epoch [5/20], Loss: 1.4377\n",
            "Epoch [6/20], Loss: 1.1976\n",
            "Epoch [7/20], Loss: 1.1710\n",
            "Epoch [8/20], Loss: 1.9656\n",
            "Epoch [9/20], Loss: 0.9988\n",
            "Epoch [10/20], Loss: 0.4608\n",
            "Epoch [11/20], Loss: 1.3251\n",
            "Epoch [12/20], Loss: 0.2247\n",
            "Epoch [13/20], Loss: 1.0927\n",
            "Epoch [14/20], Loss: 0.7478\n",
            "Epoch [15/20], Loss: 0.1771\n",
            "Epoch [16/20], Loss: 0.3652\n",
            "Epoch [17/20], Loss: 1.3301\n",
            "Epoch [18/20], Loss: 0.9416\n",
            "Epoch [19/20], Loss: 0.4896\n",
            "Epoch [20/20], Loss: 0.3308\n",
            "Test Accuracy of the model on the test samples: 67.25 %\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from google.colab import drive\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "def transform(sample):\n",
        "    \"\"\"\n",
        "    Fonction pour transformer un échantillon en un tenseur et son label\n",
        "    :param sample: le dataframe de l'échantillon a transformer\n",
        "    :return: les features et le label\n",
        "    \"\"\"\n",
        "    features = torch.tensor(sample.iloc[:-1].values.astype(np.float32))\n",
        "    target = torch.tensor(sample.iloc[-1], dtype=torch.long)\n",
        "    return features, target\n",
        "\n",
        "def normalize_data(features):\n",
        "    \"\"\"\n",
        "    Fonction pour normaliser les features en utilisant la moyenne\n",
        "    :param features: les features à normaliser\n",
        "    :return: les features avec leurs valeurs normalisées\n",
        "    \"\"\"\n",
        "    mean = features.mean()\n",
        "    std = features.std()\n",
        "    normalized_features = (features - mean) / std\n",
        "    return normalized_features\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Un Dataset personnalisé pour charger et traiter les données à partir d'un fichier CSV.\n",
        "    Les données peuvent être transformées et normalisées selon des fonctions fournies.\n",
        "    \"\"\"\n",
        "    def __init__(self, csv_file, transform=None, normalize=None):\n",
        "        \"\"\"\n",
        "        Initialise le dataset en chargeant des données à partir d'un fichier CSV, encode les labels,\n",
        "        et applique les fonctions de transformation et de normalisation si fournies.\n",
        "        :param csv_file: Chemin du fichier CSV contenant les données.\n",
        "        :param transform: Fonction optionnelle pour transformer les échantillons.\n",
        "        :param normalize: Fonction optionnelle pour normaliser les features numériques.\n",
        "        \"\"\"\n",
        "        # Chargement des données à partir du fichier CSV\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.label_encoder = LabelEncoder()\n",
        "\n",
        "        # Encodage des labels pour convertir de catégorique à numérique\n",
        "        self.data['label'] = self.label_encoder.fit_transform(self.data['label'])\n",
        "\n",
        "        # Identification et traitement des colonnes numériques\n",
        "        numerical_cols = self.data.columns[self.data.dtypes != 'object'].tolist()\n",
        "        numerical_cols = [col for col in numerical_cols if col != 'label']\n",
        "        # Remplissage des valeurs manquantes par la moyenne de chaque colonne\n",
        "        self.data[numerical_cols] = self.data[numerical_cols].apply(lambda x: x.fillna(x.mean()), axis=0)\n",
        "\n",
        "        self.transform = transform  # Fonction de transformation à appliquer aux données\n",
        "        self.normalize = normalize  # Fonction de normalisation à appliquer aux features\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Retourne la taille du dataset.\n",
        "        \"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Récupère un échantillon et son label par index, applique transformation et normalisation, et retourne le résultat.\n",
        "        :param idx: Index de l'échantillon à récupérer.\n",
        "        :return: Un tuple contenant les features normalisées et le label de l'échantillon.\n",
        "        \"\"\"\n",
        "        sample = self.data.iloc[idx]\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "        if self.normalize:\n",
        "            sample = (self.normalize(sample[0]), sample[1])\n",
        "        return sample\n",
        "\n",
        "\n",
        "class SimpleNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Réseau de neurones simple avec une couche cachée.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        \"\"\"\n",
        "        Initialise le réseau.\n",
        "        :param input_size: taille des features d'entrée.\n",
        "        :param hidden_size: taille de la couche cachée.\n",
        "        :param num_classes: nombre de classes pour la sortie. (ici nombre d'activités)\n",
        "        \"\"\"\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size) # couche linéaire cachée\n",
        "        self.relu = nn.ReLU() # fonction d'activation ReLU\n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes) # couche linéaire de sortie\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Propagation avant du réseau.\n",
        "        :param x: entrée du réseau.\n",
        "        :return: sortie du réseau.\n",
        "        \"\"\"\n",
        "        out = self.fc1(x) # couche linéaire cachée\n",
        "        out = self.relu(out) # fonction d'activation ReLU\n",
        "        out = self.fc2(out) # couche linéaire de sortie\n",
        "        return out\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, criterion, optimizer, num_epochs):\n",
        "    \"\"\"\n",
        "    Fonction pour entraîner le modèle.\n",
        "    :param model: le modèle à entraîner.\n",
        "    :param device: périphérique de calcul (GPU ou CPU).\n",
        "    :param train_loader: DataLoader pour les données d'entraînement.\n",
        "    :param criterion: fonction de perte.\n",
        "    :param optimizer: optimiseur pour la mise à jour des poids du modèle.\n",
        "    :param num_epochs: nombre total d'époques pour l'entraînement.\n",
        "    \"\"\"\n",
        "    model.train() # on met le modèle en mode entraînement\n",
        "    # on boucle sur les époques\n",
        "    for epoch in range(num_epochs):\n",
        "        # on boucle sur les batchs\n",
        "        for i, (features, labels) in enumerate(train_loader):\n",
        "            # on envoie les données sur le périphérique de calcul\n",
        "            features = features.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # propagation avant\n",
        "            outputs = model(features)\n",
        "            # calcul de la perte\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # rétropropagation et optimisation\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # affichage de la perte à chaque époque\n",
        "        print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    \"\"\"\n",
        "    Fonction pour tester le modèle sur des données de test.\n",
        "    :param model: modèle à tester.\n",
        "    :param device: périphérique de calcul (GPU ou CPU).\n",
        "    :param test_loader: DataLoader pour les données de test.\n",
        "    \"\"\"\n",
        "    model.eval() # on met le modèle en mode évaluation\n",
        "    correct = 0 # nombre de prédictions correctes\n",
        "    total = 0 # nombre total de prédictions\n",
        "    with torch.no_grad():\n",
        "        # on boucle sur les batchs\n",
        "        for features, labels in test_loader:\n",
        "            # on envoie les données sur le périphérique de calcul\n",
        "            features = features.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # propagation avant\n",
        "            outputs = model(features)\n",
        "\n",
        "            # on récupère la prédiction\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item() # on incrémente le nombre de prédictions correctes\n",
        "\n",
        "    # on calcul et on affiche la précision du modèle (pourcentage de prédictions correctes dans l'ensemble de test)\n",
        "    accuracy = 100 * correct / total\n",
        "    print('Test Accuracy of the model on the test samples: {:.2f} %'.format(accuracy))\n",
        "    return accuracy\n",
        "\n",
        "# initialisation du dataset pour les données des activités labelisées provenant du fichier CSV\n",
        "custom_dataset = CustomDataset(\"/content/drive/My Drive/data.csv\",transform=transform, normalize=normalize_data)\n",
        "\n",
        "# on divise le dataset en données d'entraînement et de test\n",
        "train_size = int(0.5 * len(custom_dataset)) # 50% des données pour l'entraînement\n",
        "test_size = len(custom_dataset) - train_size # le reste des données pour le test\n",
        "train_dataset, test_dataset = random_split(custom_dataset, [train_size, test_size]) # division aléatoire\n",
        "\n",
        "# initialisation des DataLoader pour les données d'entraînement et de test\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "# initialisation du périphérique de calcul, on utilise le GPU s'il est disponible\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# initialisation des paramètres du modèle\n",
        "input_size = custom_dataset[0][0].shape[0] # taille des features d'entrée\n",
        "hidden_size = 100 # taille de la couche cachée\n",
        "num_classes = len(custom_dataset.label_encoder.classes_) # nombre de classes pour la sortie\n",
        "num_epochs = 20 # nombre d'époques pour l'entraînement\n",
        "learning_rate = 0.001 # taux d'apprentissage\n",
        "\n",
        "# initialisation du modèle, de la fonction de perte et de l'optimiseur\n",
        "model = SimpleNet(input_size, hidden_size, num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# entraînement du modèle\n",
        "train(model, device, train_loader, criterion, optimizer, num_epochs)\n",
        "\n",
        "# test du modèle\n",
        "accuracy = test(model, device, test_loader)\n",
        "\n",
        "# sauvegarde du modèle avec le nom du fichier contenant les paramètres utilisés et la précision du modèle\n",
        "filename = f\"/content/drive/My Drive/model_projet_{hidden_size}_{num_epochs}_{learning_rate}_{round(accuracy, 2)}.pth\"\n",
        "torch.save(model.state_dict(), filename)"
      ]
    }
  ]
}